{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:35.555312Z",
     "start_time": "2020-06-11T07:24:34.239330Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_snake\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "# from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:35.560665Z",
     "start_time": "2020-06-11T07:24:35.557864Z"
    }
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:35.568840Z",
     "start_time": "2020-06-11T07:24:35.563291Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)\n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=4)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:35.573924Z",
     "start_time": "2020-06-11T07:24:35.571360Z"
    }
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:35.580550Z",
     "start_time": "2020-06-11T07:24:35.575929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=2, action=3, next_state=1, reward=4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Experience(2,3,1,4)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:35.858963Z",
     "start_time": "2020-06-11T07:24:35.852599Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:36.428371Z",
     "start_time": "2020-06-11T07:24:36.423652Z"
    }
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:36.707984Z",
     "start_time": "2020-06-11T07:24:36.698816Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "    \n",
    "    def select_action(self, state, policy_net, direction):\n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if rate > random.random():\n",
    "            if direction == 0:\n",
    "                action_space = [0,2,3]\n",
    "            elif direction == 1:\n",
    "                action_space = [1,2,3]\n",
    "            elif direction == 2:\n",
    "                action_space = [0,1,2]\n",
    "            elif direction == 3:\n",
    "                action_space = [0,1,3]\n",
    "                \n",
    "            action = np.random.choice(action_space)\n",
    "#             action = random.randrange(self.num_actions) # explore\n",
    "            return torch.tensor([action]).to(device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(device) # exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:53:37.269855Z",
     "start_time": "2020-06-11T07:53:37.245168Z"
    }
   },
   "outputs": [],
   "source": [
    "class SnakeEnvManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('snake-v0')\n",
    "        self.env.reset(width=10, height=10)\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "        self.next_state = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.env.reset(width=10, height=10)\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            return self.env.render(mode)\n",
    "        else:\n",
    "            render = self.env.game.get_current_state()\n",
    "            render = np.dstack([render,render,render])\n",
    "            return render\n",
    "    \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "    def take_action(self, action):\n",
    "        last_state, action, self.next_state, reward = self.env.step(action.item())\n",
    "        if reward == -1:\n",
    "            self.done = True\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2\n",
    "        \n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render('rgb_array').transpose((2,0,1)) \n",
    "        return self.transform_screen_data(screen)\n",
    "\n",
    "    \n",
    "    def transform_screen_data(self, screen):\n",
    "        # convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((self.get_screen_height(),self.get_screen_width())),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a battch dimension\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        return self.env.height + 2\n",
    "        \n",
    "    def get_screen_width(self):\n",
    "        return self.env.width + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:40.104507Z",
     "start_time": "2020-06-11T07:24:40.099761Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    \n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    \n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QValue Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:41.092861Z",
     "start_time": "2020-06-11T07:24:41.084561Z"
    }
   },
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # static methods can be called without creating an instance of the class\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "#         final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(-1).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:41.417843Z",
     "start_time": "2020-06-11T07:24:41.414929Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T08:15:44.263288Z",
     "start_time": "2020-06-11T07:53:39.902039Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Episode: 1000\n",
      "Average # of Actions per Game: 11.39\n",
      "Current Score: 0\n",
      "Best Score: 3\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "# define classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "em = SnakeEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "# torch nn networks\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() # not in training mode\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "# run episodes\n",
    "episode_durations = []\n",
    "all_actions = []\n",
    "food_per_game = []\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    action_list = []\n",
    "    \n",
    "    # main loop\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net, em.env.game.snake.direction)\n",
    "        action_list.append((em.env.game.snake.direction,action.item()))\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values*gamma) + rewards\n",
    "            \n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)\n",
    "            all_actions.append(action_list)\n",
    "            food_per_game.append(em.env.game.snake.length-3)\n",
    "#             plot(episode_durations, 100)\n",
    "            break\n",
    "    \n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        show = em.env.game.get_current_state()\n",
    "        plt.imshow(show)\n",
    "        plt.axis('off')\n",
    "        plt.pause(0.05)\n",
    "        display.clear_output(wait=True)\n",
    "        print(f'Current Episode: {episode+1}')\n",
    "        print(f'Average # of Actions per Game: {round(np.mean([len(i) for i in all_actions]),2)}')\n",
    "        print(f'Current Score: {em.env.game.snake.length-3}')\n",
    "        print(f'Best Score: {np.array(food_per_game).max()}') if len(food_per_game) > 0 else \\\n",
    "        print(f'Best Score: 0')\n",
    "        \n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# em.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-11T08:17:35.066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Episode: 2031\n",
      "Average # of Actions per Game: 21.28\n",
      "Current Score: 0\n",
      "Best Score: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAADLklEQVR4nO3csU3DUBRAUYIyQiqmgDITsIlFzSDUKJswQUqYgso7mAWsCKTYvkHnlIn0/2+unuTi7aZpugN67rd+ADBPnBAlTogSJ0SJE6L2l/58ennzKRcW9vn+upv73eSEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKiLmxCu5XA6r3ENrGocjoueb3JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtQqmxA+vr+ucs7zw+NVzoFbYHJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtQqmxBsMIC/MzkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR+60fQNM4HLd+wmIOp/PWT/gVkxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTomxCYFG3snWgyOSEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihCibEJhlg8H2TE6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUfo1LxuG4xjXwr5icECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBC1m6Zp6zcAM0xOiBInRIkTosQJUeKEKHFC1A9MIRnd4ltOjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for episode in range(1000, 10000):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    action_list = []\n",
    "    \n",
    "    # main loop\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net, em.env.game.snake.direction)\n",
    "        action_list.append((em.env.game.snake.direction,action.item()))\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values*gamma) + rewards\n",
    "            \n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)\n",
    "            all_actions.append(action_list)\n",
    "            food_per_game.append(em.env.game.snake.length-3)\n",
    "#             plot(episode_durations, 100)\n",
    "            break\n",
    "    \n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        show = em.env.game.get_current_state()\n",
    "        plt.imshow(show)\n",
    "        plt.axis('off')\n",
    "        plt.pause(0.05)\n",
    "        display.clear_output(wait=True)\n",
    "        print(f'Current Episode: {episode+1}')\n",
    "        print(f'Average # of Actions per Game: {round(np.mean([len(i) for i in all_actions]),2)}')\n",
    "        print(f'Current Score: {em.env.game.snake.length-3}')\n",
    "        print(f'Best Score: {np.array(food_per_game).max()}') if len(food_per_game) > 0 else \\\n",
    "        print(f'Best Score: 0')\n",
    "        \n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env]",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
